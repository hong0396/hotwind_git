一般来说，Feature应该是informative（富有信息量），discriminative（有区分性）和independent（独立）

数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。
通常来说，从两个方面考虑来选择特征：
特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用
特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。




根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。　　我们使用sklearn中的feature_selection库来进行特征选择。

作者：城东
链接：https://www.zhihu.com/question/28641663/answer/110165221



作者：城东
链接：https://www.zhihu.com/question/28641663/answer/110165221


我觉得，稀疏表达的作用有两个：
1） 降低数据维数，稀疏表达后的特征向量各维之间的依赖性变低，更为独立
2）从人工智能的角度来看，这更加接近“自动分析出隐藏在数据背后的解释因子”这一思想稀疏表达求取时所加的稀疏约束，
使得计算后得到的各个“基“对于解释数据具有相同的重要性，其目的正是在尝试找出隐藏在数据背后的解释因子
