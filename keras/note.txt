神经网络中的每个神经元 对其所有的输入进行加权求和，并添加一个被称为偏置（bias） 的常数，然后通过一些非线性激活函数来反馈结果。



softmax主要用来做多分类问题，是logistic回归模型在多分类问题上的推广
softmax一般作为神经网络最后一层，作为输出层进行多分类，Softmax的输出的每个值都是>=0，并且其总和为1，所以可以认为其为概率分布。

损失函数：
损失函数（loss function），是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数，在统计学中损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。


交叉熵（cross-entropy）就是神经网络中常用的损失函数。

一个比较简单的理解就是使得 预测值Yi和真实值Y' 对接近，即两者的乘积越大，coss-entropy越小。




梯度下降：
如果对于所有的权重和所有的偏置计算交叉熵的偏导数，就得到一个对于给定图像、标签和当前权重和偏置的「梯度」，如图所示：

我们希望损失函数最小，也就是需要到达交叉熵最小的凹点的低部。在上图中，交叉熵被表示为一个具有两个权重的函数。

而学习速率，即在梯度下降中的步伐大小。


















